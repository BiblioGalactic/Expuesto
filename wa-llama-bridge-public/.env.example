# =========================
# WhatsApp -> llama.cpp bridge
# =========================

# LLM (OpenAI-compatible endpoint from llama-server)
LLM_BASE_URL=http://127.0.0.1:8080/v1
LLM_MODEL=/absolute/path/to/your/model.gguf
LLM_API_KEY=

# Optional fallback LLM (same machine or another machine)
LLM_FALLBACK_BASE_URL=
LLM_FALLBACK_MODEL=
LLM_FALLBACK_API_KEY=

# Generation
# If SYSTEM_PROMPT_FILE exists, it has priority over SYSTEM_PROMPT.
SYSTEM_PROMPT_FILE=./prompts/prompt_main.txt
SYSTEM_PROMPT=You are a helpful assistant running through WhatsApp.
TEMPERATURE=0.8
MAX_TOKENS=500
REQUEST_TIMEOUT_MS=120000

# Memory (per chat)
HISTORY_TURNS=8
MAX_HISTORY_CHARS=12000

# Routing / safety
SELF_CHAT_ONLY=true
ALLOW_GROUPS=false
ALLOW_FROM=+15555550123
IGNORE_OLD_MESSAGES=true

# WhatsApp login mode
# true = pairing code; false = QR
WA_USE_PAIRING_CODE=true
WA_PAIRING_PHONE=15555550123

# QR mode (only used if WA_USE_PAIRING_CODE=false)
WA_SHOW_QR=false

# Delivery
WA_REPLY_CHUNK_CHARS=1400

# Logs: silent | fatal | error | warn | info | debug | trace
LOG_LEVEL=info
